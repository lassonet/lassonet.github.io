<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <!-- <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /> -->
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>LassoNet: Deep Lasso-Selection of 3D Point Clouds</title>
   
    <script src="//code.jquery.com/jquery-3.3.1.min.js"></script>

    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>
    <link rel="stylesheet" href="styles/tomorrow-night.css">
    <script src="js/highlight.pack.js"></script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML' async></script>
</head>

<body>
    <div class="teaser">
        <img src="./figs/teaser.jpg" />
        <div class="mask"></div>
        <div class="teaser-content">
            <h1>LassoNet: Deep Lasso-Selection of 3D Point Clouds</h1>
            <div class="authors">
                <span>
                    <a href="//chenzhutian.org" target="_blank">Zhutian Chen</a>, <a href="//zeng-wei.com" target="_blank">Wei Zeng</a>, <a href="//github.com/zhiguangyang" target="_blank">Zhiguang Yang</a>, <a href="//yulingyun.com/" target="_blank">Lingyun Yu</a>, <a href="//www.cse.cuhk.edu.hk/~cwfu" target="_blank">Chi-Wing Fu</a>, and <a href="//huamin.org" target="_blank">Huamin Qu</a>
                </span>
                <span>
                    <a href="//ieeevis.org/year/2019/info/papers-sessions" target="_blank">IEEE VIS 2019</a>
                </span>
            </div>
            <!-- <div class="venue">
                IEEE VIS 2019
            </div> -->
            <div class="jump-btn-group">
                <!-- <a href="#abstract" class="jump-btn">Abstract</a> -->
                <!-- <a href="#labels" class="jump-btn">Labels</a>
                <a href="#architecture" class="jump-btn">Architecture</a>
                <a href="#training" class="jump-btn">Training</a>
                <a href="#examples" class="jump-btn">Examples</a> -->
                <a href="#architecture" class="jump-btn">Pipeline</a> 
                <a href="#dataset" class="jump-btn">Dataset</a>                 
                <a href="#examples" class="jump-btn">Examples</a> 
                <a href="//chenzhutian.org/projects/2019_lassonet/paper.pdf" class="jump-btn" target="_blank">Paper</a>
                <!-- <a href="#materials" class="jump-btn">Materials</a> -->
            </div>
        </div>
    </div>
    <div class="content" style="width:80vw;max-width: 1024px;">
        <section id="abstract" class="abstract">
            <h2>Abstract</h2>
            Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection
            methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific
            challenges root in the great variabilities implied by point clouds (<em>e.g.</em>, dense vs. sparse), viewpoint (<em>e.g.</em>, occluded vs. non-occluded),
            and lasso (<em>e.g.</em>, small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds,
            attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points
            with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an
            intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on
            two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods.
            The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point
            clouds, viewpoints, and lasso selections.
        </section>
        <hr />

        <section id="architecture" class="architecture">
            <h2>Pipeline and Network Architecture</h2>

            <figure style="text-align: center;
            width: 100%;
            margin: 0 auto 20px auto;">
                <img src="figs/pipeline.jpg" alt="missing">
                <figcaption>
                    Figure 3. <strong>Pipeline</strong>. LassoNet consists of three stages: In <em>Interaction Encoding</em> stage, we associate point cloud with viewpoint and lasso through 3D coordinate transformation and naive selection; 
                    In <em>Filtering and Sampling</em> stage, we reduce the amount of points for network processing through intention filtering
                    and farthest point sampling. 
                    Lastly, we build a hierarchical neural network in <em>Network Building</em> stage.
                </figcaption>
            </figure>

            <figure style="text-align: center;
            width: 100%;
            margin: 0 auto 20px auto;">
                <img src="figs/arch.jpg" alt="missing">
                <figcaption>
                    Figure 4. <strong>Overview of network building</strong>. The DNN network is built upon (a) PointNet <cite>ref_pn</cite>, 
                    and we employ a hierarchical structure that generates more local and global features using 
                    (b) abstraction and (c) propagation components.
                </figcaption>
            </figure>

        </section>

        <hr />
        <section id="dataset" class="dataset">
            <h2>Dataset</h2>

            <figure style="text-align: center;
            width: 100%;
            margin: 0 auto 20px auto;">
                <img src="figs/records.jpg" alt="missing">
                <figcaption>
                    Figure 5. <strong>Exemplar annotation records</strong> for point clouds in ShapeNet (left)
                    and S3DIS (right): target and interfering points are colored in yellow and
                    blue respectively, while lassos are in red color.
                </figcaption>
            </figure>

            <table id="tb_dataset" style="width:50%;">
                    <thead>
                        <tr>
                            <th style="width:157px;">Dataset</th>
                            <th>#Point Clouds</th>
                            <th>#Targets</th>
                            <th>#Records</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>ShapeNet</th>
                            <td>2,332</td>
                            <td>6,297</td>
                            <td>19,432</td>
                        </tr>
                        <tr>
                            <td>S3DIS</td>
                            <td>2,72</td>
                            <td>4,018</td>
                            <td>12,944</td>
                        </tr>
                    </tbody>
                    <caption>Statistics of lasso-selection records.</caption>
                </table>

                <p>
                    The table  presents statistics of lasso-selection records. In total, we
                    have collected 19,432 lasso-selection records for 6,297 different parts
                    of target points in ShapeNet point clouds, and 12,944 records for 4,018
                    different parts of target points in S3DIS point clouds.
                </p>

        </section>

        <hr />
        <section id="examples" class="examples">
            <h2>Examples</h2>

            <video style="text-align: center;
            width: 100%;
            margin: 0 auto 20px auto;" controls>
            <source src="figs/example.mp4" type="video/mp4">
            </video>

        </section>

        <hr />
        <section id="references" class="references">
            <h2 style="text-align:left;">Reference</h2>
            <p id="ref_pn">Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas. <a
                    href="https://arxiv.org/abs/1612.00593">"PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation."</a> In Proc. IEEE CVPR. 2017.</p>
        </section>
    </div>

    <script>
        function insertExampleImage() {
            const section = document.querySelector('#examples > #examples_good')
            for (let i = 1; i < 13; ++i) {
                section.insertAdjacentHTML('beforeend', `
                <div class="item">
                    <div class="item-content">
                    <a data-fancybox="gallery" href="figs/examples/${i}_u.png">
                    <img src="figs/examples/${i}_u_tn.jpg" alt='missing' /> </a>
                    </div>
                </div>
                <div class="item">
                    <div class="item-content">
                        <a data-fancybox="gallery" href="figs/examples/${i}_d.png">
                            <img src="figs/examples/${i}_d_tn.jpg" alt='missing' />
                        </a>
                    </div>
                </div>
                `)
            }
        }

        function insertCategoryTable() {
            const target = document.querySelector('#tb_categories > tbody')
            const tableData = [
                ['<img style="width:15px;" src="https://placehold.it/15/FFC000/000000?text=+" /> Event mark', 'A graphical mark that represents an event. The mark does not relate to the content of the event it represents.', 'BBox + Mask', '1 / event'],
                ['<img style="width:15px;" src="https://placehold.it/15/34495E/000000?text=+" /> Event text', 'A block of text that depicts and only depicts the occurred time of an event.', 'BBox', '0 ~ 1 / event'],
                ['<img style="width:15px;" src="https://placehold.it/15/5B9BD5/000000?text=+" /> Annotation mark', 'A graphical mark that annotates an event. The mark does not relate to the content of the event it annotates.', 'BBox + Mask', '0 ~ n / event'],
                ['<img style="width:15px;" src="https://placehold.it/15/AF7AC4/000000?text=+" /> Annotation text', 'A block of text that depicts the content of an event. The occurred time of the event can be included.', 'BBox', '0 ~ n / event'],
                ['<img style="width:15px;" src="https://placehold.it/15/FC6868/000000?text=+" /> Annotation icon', 'A graphical or natural image that annotates an event.', 'BBox', '0 ~ n / event'],
                ['<img style="width:15px;" src="https://placehold.it/15/1ABC9C/000000?text=+" /> Main body', 'A graphical mark that represents the time.', 'BBox + Mask', '0 ~ n / image'],
            ]
            for (const line of tableData) {
                target.insertAdjacentHTML('beforeend', `<tr>${line.map(d => `<td>${d}</th>`).join('')}</td>`)
            }
        }

        function insertLossesTable() {
            const target = document.querySelector('#tb_losses > tbody')
            const data = [
                [String.raw`\(\mathcal{L}_{{Image}_{type}}\)`, 'Image', 'Classification', 'Cross-Entropy', 0.15],
                [String.raw`\(\mathcal{L}_{{Image}_{orientation}}\)`, 'Image', 'Classification', 'Cross-Entropy', 0.15],
                [String.raw`\(\mathcal{L}_{{RoI}_{objectness}}\)`, 'RoI', 'Classification', 'Cross-Entropy', 1],
                [String.raw`\(\mathcal{L}_{{RoI}_{bbox}}\)`, 'RoI', 'Regression', String.raw`Smooth \(L_1\)`, 1],
                [String.raw`\(\mathcal{L}_{{DT}_{type}}\)`, 'DT', 'Classification', 'Cross-Entropy', 1],
                [String.raw`\(\mathcal{L}_{{DT}_{bbox}}\)`, 'DT', 'Regression', String.raw`Smooth \(L_1\)`, 1],
                [String.raw`\(\mathcal{L}_{{DT}_{mask}}\)`, 'DT', 'Classification', 'Cross-Entropy', 1]
            ]
            for (const line of data) {
                target.insertAdjacentHTML('beforeend', `<tr>${line.map(d => `<td>${d}</th>`).join('')}</td>`)
            }
        }

        function insertPaperPreview() {
            const target = document.querySelector('#paper > .gallary')
            for (let i = 1; i < 11; ++i) {
                target.insertAdjacentHTML('beforeend', `<a href="https://chenzhutian.org/projects/2019_autotimeline/paper.pdf" class="item item-5">
                    <div class="item-content"><img src="figs/paper/${i}_tn.jpg"/>
                    </div>
                    </a>`)
            }
        }

        function convertReference() {
            const cites = document.querySelectorAll('cite')
            let id = 1
            const orderBook = {}
            cites.forEach(c => {
                if (!(c.textContent in orderBook)) {
                    orderBook[c.textContent] = id++
                }
            })
            cites.forEach(c => {
                const refId = c.textContent
                c.textContent = ''
                c.insertAdjacentHTML('beforeend', `<a href="#${refId}">[${orderBook[refId]}]</a>`)
            })

            // reorder
            const ps = Array.from(document.querySelectorAll('#references > p'))
            ps.forEach(p => {
                p.innerHTML = `[${orderBook[p.id]}] ${p.innerHTML}`
                p.parentElement.removeChild(p)
            })
            const parent = document.querySelector('#references')
            ps.filter(p => orderBook[p.id])
                .sort((a, b) => orderBook[a.id] - orderBook[b.id])
                .forEach(p => parent.appendChild(p))
        }

        // // insert categorytable
        // insertCategoryTable()

        // // insert examples
        // insertExampleImage()

        // // insert losses
        // insertLossesTable()

        // insert paper
        // insertPaperPreview()

        // highlight code
        // hljs.initHighlightingOnLoad()

        // reference
        convertReference()

    </script>
</body>

</html>